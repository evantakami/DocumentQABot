import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)
import os
import logging
import pandas as pd
import tiktoken  # ç”¨äº token è®¡æ•°
import time  # ç”¨äºç­‰å¾…
import threading  # ç”¨äºç¬¬ä¸€æ¬¡æ—¥å¿—è®°å½•çš„é”
import openpyxl  # ç”¨äºæ“ä½œ Excel

# LangChain ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆlangchain ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.schema import HumanMessage

# --- Token æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° ---
def count_tokens(text, model="gpt-3.5-turbo"):
    """
    æŒ‡å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã® token æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        enc = tiktoken.encoding_for_model(model)
    except Exception:
        enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

# å…¨å±€å˜é‡åŠé”ï¼Œç”¨äºåªè®°å½•ç¬¬ä¸€æ¬¡çš„ Prompt å’Œ AI çš„ comment
first_log_done = False
first_log_lock = threading.Lock()

# 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿: ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã¨äº‹å‰æ¤œè¨¼ MD ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™
csv_file = "checklist.csv"
md_file = "äº‹å‰æ¤œè¨¼.md"

# ãƒ­ã‚°ã®è¨­å®š: INFO ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã—ã€æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™
logging.basicConfig(filename="process.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s", encoding="utf-8")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")

# ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼ˆUTF-8 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™ï¼‰
try:
    df = pd.read_csv(csv_file, encoding="utf-8-sig")  # utf-8-sig ã‚’ä½¿ç”¨ã—ã¦ BOM ã‚’ã‚µãƒãƒ¼ãƒˆ
except Exception as e:
    logging.error(f"CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# Markdown ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™
try:
    with open(md_file, "r", encoding="utf-8") as f:
        doc_text = f.read()
except Exception as e:
    logging.error(f"Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

logging.info(f"ãƒã‚§ãƒƒã‚¯é …ç›®ã®æ•°: {len(df)}")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

# 2. Azure OpenAI ã®è¨­å®š: API èªè¨¼æƒ…å ±ã¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã‚’è¨­å®šã—ã¾ã™
api_key = "YOUR_AZURE_OPENAI_API_KEY"
api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com/"  # Azure OpenAI ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
api_version = "2023-05-15"  # API ãƒãƒ¼ã‚¸ãƒ§ãƒ³
deployment_name = "YOUR_DEPLOYMENT_NAME"  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå

# ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼ˆè´¹ç”¨è®¡ç®—ä¾æ®ï¼‰ï¼Œä¾‹å¦‚ "gpt-3.5-turbo" æˆ– "gpt-4"
selected_model = "gpt-3.5-turbo"

# è®¾ç½®è´¹ç”¨ï¼ˆå•ä½ï¼šç¾å…ƒ/Tokenï¼‰ï¼Œè¿™é‡Œä»…ä¸ºç¤ºä¾‹ï¼Œå…·ä½“å®šä»·è¯·å‚ç…§å®é™…æƒ…å†µ
cost_dict = {
    "gpt-3.5-turbo": 0.002 / 1000,
    "gpt-4": 0.03 / 1000
}

# èªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã™ï¼ˆLangChain ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ï¼‰
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = api_key
os.environ["OPENAI_API_BASE"] = api_base
os.environ["OPENAI_API_VERSION"] = api_version

# AzureChatOpenAI ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™
try:
    llm = AzureChatOpenAI(deployment_name=deployment_name,
                          openai_api_base=api_base,
                          openai_api_version=api_version,
                          openai_api_key=api_key,
                          openai_api_type="azure",
                          temperature=0)
    logging.info("Azure OpenAI LLM ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Azure OpenAI ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# 3. å¤šæ®µéšæ¤œæŸ»ã«å¿…è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®šç¾©ã—ã¾ã™

# ã‚¹ãƒ†ãƒ¼ã‚¸1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å…¨ä½“ã«åŸºã¥ã„ã¦ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
template_stage1 = """ã‚ãªãŸã¯æ‰‹é †æ›¸ã®äº‹å‰æ¤œè¨¼ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é«˜åº¦ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦ã€ãã®é …ç›®ã«é–¢ã™ã‚‹AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§è¿°ã¹ã¦ãã ã•ã„ã€‚
ãƒã‚§ãƒƒã‚¯é …ç›®: {check_item}
æ–‡æ›¸å†…å®¹: \"\"\"{document}\"\"\"
AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ:"""
prompt_stage1 = PromptTemplate(
    input_variables=["check_item", "document"],
    template=template_stage1
)

# ã‚¹ãƒ†ãƒ¼ã‚¸2ã®å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼: JSON å½¢å¼ã§ã€ŒAIè©•ä¾¡ã€ã¨ã€Œæ”¹å–„æ¡ˆã€ã‚’å‡ºåŠ›ã™ã‚‹æ§‹é€ ã‚’å®šç¾©ã—ã¾ã™
response_schemas = [
    ResponseSchema(name="AIè©•ä¾¡", description="è©•ä¾¡çµæœã€‚OKã€NGã€ã¾ãŸã¯ã€Œ-ã€ã®ã„ãšã‚Œã‹ã€‚"),
    ResponseSchema(name="æ”¹å–„æ¡ˆ", description="è©•ä¾¡ãŒNGã®å ´åˆã€é …ç›®ã‚’æº€ãŸã™ãŸã‚ã®æ”¹å–„ææ¡ˆã€‚OKã‚„ã€Œ-ã€ã®å ´åˆã¯ç©ºæ–‡å­—ã€‚")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

# ã‚¹ãƒ†ãƒ¼ã‚¸2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€æ§‹é€ åŒ–ã•ã‚ŒãŸè©•ä¾¡çµæœã¨æ”¹å–„æ¡ˆã‚’å‡ºåŠ›ã—ã¾ã™
template_stage2 = """ã‚ãªãŸã¯ãƒã‚§ãƒƒã‚¯çµæœã‚’åˆ¤å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€ãƒã‚§ãƒƒã‚¯é …ç›®ã®æœ€çµ‚è©•ä¾¡ã¨æ”¹å–„æ¡ˆã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
è©•ä¾¡ã¯ã€ŒOKã€ã€ŒNGã€ã¾ãŸã¯ã€Œ-ã€ã§è¡¨ã—ã€NGã®å ´åˆã¯æ”¹å–„æ¡ˆã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚
{format_instructions}
AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ: {ai_comment}"""
prompt_stage2 = PromptTemplate(
    input_variables=["ai_comment"],
    partial_variables={"format_instructions": format_instructions},
    template=template_stage2
)

def process_check_item(item):
    """
    å„ãƒã‚§ãƒƒã‚¯é …ç›®ã«å¯¾ã—ã¦ã€äºŒæ®µéšã§LLMã‚’å‘¼ã³å‡ºã—ã€
    ã€ŒAIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã€ã¨ã€ŒAIè©•ä¾¡ã€ãŠã‚ˆã³ã€Œæ”¹å–„æ¡ˆã€ã‚’ç”Ÿæˆã—ã€ã•ã‚‰ã« Token æ¶ˆè²»æ•°ã‚‚è¨ˆç®—ã—ã¦è¿”ã—ã¾ã™ã€‚
    è¿™é‡Œå¢åŠ  total_tokens å­—æ®µç”¨äºè´¹ç”¨ç»Ÿè®¡ï¼Œä½†åç»­è¾“å‡ºExcelæ—¶ä¸åŒ…å« Token æ¶ˆè€—åˆ—ã€‚
    """
    global first_log_done
    item_no = item.get("é …ç•ª", "<no-id>")
    content = str(item.get("ç¢ºèªå†…å®¹", ""))
    
    # --- ã‚¹ãƒ†ãƒ¼ã‚¸1: AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ ---
    try:
        comment_prompt = prompt_stage1.format(check_item=content, document=doc_text)
        ai_comment = llm([HumanMessage(content=comment_prompt)]).content
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        tokens_prompt1 = count_tokens(comment_prompt)
        tokens_response1 = count_tokens(ai_comment)
        
        # ä»…åœ¨ç¬¬ä¸€æ¬¡å¤„ç†æ—¶ï¼Œå°†ç”¨æˆ·ç»™çš„ prompt å’Œ AI çš„ comment è¿½åŠ åˆ°æ—¥å¿—ä¸­
        with first_log_lock:
            if not first_log_done:
                logging.info("Userç»™çš„Promptå†…å®¹: " + comment_prompt)
                logging.info("AIçš„commentå†…å®¹: " + ai_comment)
                first_log_done = True
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {
            "é …ç•ª": item_no,
            "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "AIè©•ä¾¡": "-",
            "æ”¹å–„æ¡ˆ": "",
            "Tokenæ¶ˆè²»": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "total_tokens": 0
        }
    
    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2: è©•ä¾¡çµæœã¨æ”¹å–„æ¡ˆã®ç”Ÿæˆ ---
    try:
        result_prompt = prompt_stage2.format(ai_comment=ai_comment)
        raw_output = llm([HumanMessage(content=result_prompt)]).content
        result = output_parser.parse(raw_output)
        ai_hyouka = result.get("AIè©•ä¾¡", "")
        kaizen = result.get("æ”¹å–„æ¡ˆ", "")
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ (è©•ä¾¡: {ai_hyouka})")
        tokens_prompt2 = count_tokens(result_prompt)
        tokens_response2 = count_tokens(raw_output)
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”Ÿæˆã¾ãŸã¯è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        ai_hyouka = "-"
        kaizen = f"ã‚¨ãƒ©ãƒ¼: {e}"
        tokens_prompt2 = 0
        tokens_response2 = 0
    
    total_tokens = tokens_prompt1 + tokens_response1 + tokens_prompt2 + tokens_response2
    token_info = (f"Stage1: prompt {tokens_prompt1}, response {tokens_response1}; "
                  f"Stage2: prompt {tokens_prompt2}, response {tokens_response2}; "
                  f"åˆè¨ˆ: {total_tokens}")
    
    return {
        "é …ç•ª": item_no,
        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": ai_comment,
        "AIè©•ä¾¡": ai_hyouka,
        "æ”¹å–„æ¡ˆ": kaizen,
        "Tokenæ¶ˆè²»": token_info,
        "total_tokens": total_tokens
    }

# 5. ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’é †æ¬¡å‡¦ç†ã—ã¾ã™ï¼ˆå–æ¶ˆå¹¶è¡Œå¤„ç†ï¼‰
logging.info("ãƒã‚§ãƒƒã‚¯é …ç›®ã®å‡¦ç†ã‚’é †æ¬¡å®Ÿè¡Œã—ã¾ã™")
items = df.to_dict(orient="records")
results = []
total_tasks = len(items)
total_tokens_sum = 0

for idx, item in enumerate(items):
    result = process_check_item(item)
    results.append(result)
    total_tokens_sum += result.get("total_tokens", 0)
    print(f"é€²æ—: {idx+1}/{total_tasks} å®Œäº†")
    # æ¯ä¸ªä»»åŠ¡é—´å¯é€‚å½“ç­‰å¾…ï¼ˆå¦‚éœ€ï¼‰ï¼š
    time.sleep(1)

# æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹è¿›è¡Œè´¹ç”¨è®¡ç®—ï¼ˆä¸åæ˜ åœ¨Excelä¸Šï¼Œä»…è®°å½•æ—¥å¿—ï¼‰
total_cost = total_tokens_sum * cost_dict.get(selected_model, 0)
logging.info(f"ä½¿ç”¨æ¨¡å‹ {selected_model} ç·Tokenæ¶ˆè²»: {total_tokens_sum}, è´¹ç”¨: ${total_cost:.6f}")

# 6. çµæœã‚’å…ƒã® DataFrame ã«çµ±åˆã—ã€Excel ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåŸExcelã®ã‚³ãƒ”ãƒ¼ï¼‰ã«å‡ºåŠ›ã—ã¾ã™
# è¿™é‡Œåªå–éœ€è¦çš„3ä¸ªå­—æ®µï¼Œä¸åŒ…å« Tokenæ¶ˆè²» åˆ—
for col in ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ"]:
    df[col] = df["é …ç•ª"].map({r["é …ç•ª"]: r[col] for r in results})

# åŸExcelæ–‡ä»¶åç§°
original_excel = "F-0168-2.xlsx"
# å¤„ç†åExcelæ–‡ä»¶åç§°ï¼ˆä¿ç•™åŸæ–‡ä»¶ï¼Œä»…å¯¹å¤åˆ¶æ–‡ä»¶è¿›è¡Œå¤„ç†ï¼‰
output_excel = "F-0168-2_Processed.xlsx"

# è¯»å–åŸExcelæ–‡ä»¶ï¼Œå¹¶å®šä½åˆ°æŒ‡å®šå·¥ä½œè¡¨
wb = openpyxl.load_workbook(original_excel)
ws = wb["å•†ç”¨ä½œæ¥­æ‰‹é †æ›¸ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ"]

# åœ¨ Håˆ—(åˆ—8)å’Œ Jåˆ—(åˆ—10)ä¹‹é—´æ’å…¥3åˆ—ï¼ˆå³åœ¨ç¬¬9åˆ—å¼€å§‹æ’å…¥3åˆ—ï¼‰
ws.insert_cols(idx=9, amount=3)

# å®šä¹‰æ–°æ’å…¥åˆ—çš„æ ‡é¢˜ï¼ˆæ ‡é¢˜è¡Œåœ¨ç¬¬8è¡Œï¼‰ï¼Œå¹¶å¤åˆ¶ Håˆ—ï¼ˆåˆ—8ï¼‰å•å…ƒæ ¼çš„æ ¼å¼
new_headers = ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ"]

def copy_cell_style(source, target):
    target.font = source.font
    target.border = source.border
    target.fill = source.fill
    target.number_format = source.number_format
    target.protection = source.protection
    target.alignment = source.alignment

# å¤åˆ¶æ ‡é¢˜è¡Œæ ¼å¼ï¼ˆå‡è®¾ H8 æœ‰åˆé€‚æ ¼å¼ï¼‰
source_cell = ws.cell(row=8, column=8)
for offset, header in enumerate(new_headers, start=0):
    cell = ws.cell(row=8, column=9+offset)
    cell.value = header
    copy_cell_style(source_cell, cell)

# å‡è®¾Excelä¸­çš„æ•°æ®ä»ç¬¬9è¡Œå¼€å§‹ï¼Œä¸CSVä¸­æ•°æ®è¡Œé¡ºåºä¸€è‡´
# å°†å¤„ç†ç»“æœå†™å…¥æ–°æ’å…¥çš„3åˆ—ï¼ˆç¬¬9åˆ—~ç¬¬11åˆ—ï¼‰ï¼Œå¹¶å¤åˆ¶å¯¹åº”è¡ŒHåˆ—çš„æ ¼å¼
start_row = 9
for i, row_data in enumerate(results):
    excel_row = start_row + i
    # å¤„ç†ç»“æœå­—æ®µä¾æ¬¡å†™å…¥åˆ— 9, 10, 11
    for col_offset, field in enumerate(new_headers, start=0):
        cell = ws.cell(row=excel_row, column=9+col_offset)
        cell.value = row_data[field]
        source_cell = ws.cell(row=excel_row, column=8)  # å¤åˆ¶Håˆ—æ ¼å¼
        copy_cell_style(source_cell, cell)

# ä¿å­˜å¤„ç†åçš„Excelæ–‡ä»¶
wb.save(output_excel)
logging.info(f"çµæœã¯ {output_excel} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

print("æ¤œæŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯", output_excel, "ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
import os
import logging
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
import tiktoken  # ç”¨äº token è®¡æ•°
import time  # ç”¨äºç­‰å¾…

# LangChain ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆlangchain ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.schema import HumanMessage

# --- Token æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° ---
def count_tokens(text, model="gpt-3.5-turbo"):
    """
    æŒ‡å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã® token æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        enc = tiktoken.encoding_for_model(model)
    except Exception:
        enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

# 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿: ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã¨äº‹å‰æ¤œè¨¼ MD ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™
csv_file = "checklist.csv"
md_file = "äº‹å‰æ¤œè¨¼.md"

# ãƒ­ã‚°ã®è¨­å®š: INFO ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã—ã€æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™
logging.basicConfig(filename="process.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s", encoding="utf-8")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")

# ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼ˆUTF-8 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™ï¼‰
try:
    df = pd.read_csv(csv_file, encoding="utf-8-sig")  # utf-8-sig ã‚’ä½¿ç”¨ã—ã¦ BOM ã‚’ã‚µãƒãƒ¼ãƒˆ
except Exception as e:
    logging.error(f"CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# Markdown ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™
try:
    with open(md_file, "r", encoding="utf-8") as f:
        doc_text = f.read()
except Exception as e:
    logging.error(f"Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

logging.info(f"ãƒã‚§ãƒƒã‚¯é …ç›®ã®æ•°: {len(df)}")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

# 2. Azure OpenAI ã®è¨­å®š: API èªè¨¼æƒ…å ±ã¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã‚’è¨­å®šã—ã¾ã™
api_key = "YOUR_AZURE_OPENAI_API_KEY"
api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com/"  # Azure OpenAI ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
api_version = "2023-05-15"  # API ãƒãƒ¼ã‚¸ãƒ§ãƒ³
deployment_name = "YOUR_DEPLOYMENT_NAME"  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå

# èªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã™ï¼ˆLangChain ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ï¼‰
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = api_key
os.environ["OPENAI_API_BASE"] = api_base
os.environ["OPENAI_API_VERSION"] = api_version

# AzureChatOpenAI ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™
try:
    llm = AzureChatOpenAI(deployment_name=deployment_name,
                          openai_api_base=api_base,
                          openai_api_version=api_version,
                          openai_api_key=api_key,
                          openai_api_type="azure",
                          temperature=0)
    logging.info("Azure OpenAI LLM ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Azure OpenAI ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# 3. å¤šæ®µéšæ¤œæŸ»ã«å¿…è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®šç¾©ã—ã¾ã™

# ã‚¹ãƒ†ãƒ¼ã‚¸1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å…¨ä½“ã«åŸºã¥ã„ã¦ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
template_stage1 = """ã‚ãªãŸã¯æ‰‹é †æ›¸ã®äº‹å‰æ¤œè¨¼ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é«˜åº¦ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦ã€ãã®é …ç›®ã«é–¢ã™ã‚‹AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§è¿°ã¹ã¦ãã ã•ã„ã€‚
ãƒã‚§ãƒƒã‚¯é …ç›®: {check_item}
æ–‡æ›¸å†…å®¹: \"\"\"{document}\"\"\"
AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ:"""
prompt_stage1 = PromptTemplate(
    input_variables=["check_item", "document"],
    template=template_stage1
)

# ã‚¹ãƒ†ãƒ¼ã‚¸2ã®å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼: JSON å½¢å¼ã§ã€ŒAIè©•ä¾¡ã€ã¨ã€Œæ”¹å–„æ¡ˆã€ã‚’å‡ºåŠ›ã™ã‚‹æ§‹é€ ã‚’å®šç¾©ã—ã¾ã™
response_schemas = [
    ResponseSchema(name="AIè©•ä¾¡", description="è©•ä¾¡çµæœã€‚OKã€NGã€ã¾ãŸã¯ã€Œ-ã€ã®ã„ãšã‚Œã‹ã€‚"),
    ResponseSchema(name="æ”¹å–„æ¡ˆ", description="è©•ä¾¡ãŒNGã®å ´åˆã€é …ç›®ã‚’æº€ãŸã™ãŸã‚ã®æ”¹å–„ææ¡ˆã€‚OKã‚„ã€Œ-ã€ã®å ´åˆã¯ç©ºæ–‡å­—ã€‚")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

# ã‚¹ãƒ†ãƒ¼ã‚¸2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€æ§‹é€ åŒ–ã•ã‚ŒãŸè©•ä¾¡çµæœã¨æ”¹å–„æ¡ˆã‚’å‡ºåŠ›ã—ã¾ã™
template_stage2 = """ã‚ãªãŸã¯ãƒã‚§ãƒƒã‚¯çµæœã‚’åˆ¤å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€ãƒã‚§ãƒƒã‚¯é …ç›®ã®æœ€çµ‚è©•ä¾¡ã¨æ”¹å–„æ¡ˆã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
è©•ä¾¡ã¯ã€ŒOKã€ã€ŒNGã€ã¾ãŸã¯ã€Œ-ã€ã§è¡¨ã—ã€NGã®å ´åˆã¯æ”¹å–„æ¡ˆã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚
{format_instructions}
AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ: {ai_comment}"""
prompt_stage2 = PromptTemplate(
    input_variables=["ai_comment"],
    partial_variables={"format_instructions": format_instructions},
    template=template_stage2
)

def process_check_item(item):
    """
    å„ãƒã‚§ãƒƒã‚¯é …ç›®ã«å¯¾ã—ã¦ã€äºŒæ®µéšã§LLMã‚’å‘¼ã³å‡ºã—ã€
    ã€ŒAIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã€ã¨ã€ŒAIè©•ä¾¡ã€ãŠã‚ˆã³ã€Œæ”¹å–„æ¡ˆã€ã‚’ç”Ÿæˆã—ã€ã•ã‚‰ã« Token æ¶ˆè²»æ•°ã‚‚è¨ˆç®—ã—ã¦è¿”ã—ã¾ã™ã€‚
    """
    item_no = item.get("é …ç•ª", "<no-id>")
    content = str(item.get("ç¢ºèªå†…å®¹", ""))
    
    # --- ã‚¹ãƒ†ãƒ¼ã‚¸1: AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆ ---
    try:
        comment_prompt = prompt_stage1.format(check_item=content, document=doc_text)
        ai_comment = llm([HumanMessage(content=comment_prompt)]).content
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        tokens_prompt1 = count_tokens(comment_prompt)
        tokens_response1 = count_tokens(ai_comment)
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {
            "é …ç•ª": item_no,
            "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "AIè©•ä¾¡": "-",
            "æ”¹å–„æ¡ˆ": "",
            "Tokenæ¶ˆè²»": f"ã‚¨ãƒ©ãƒ¼: {e}"
        }
    
    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2: è©•ä¾¡çµæœã¨æ”¹å–„æ¡ˆã®ç”Ÿæˆ ---
    try:
        result_prompt = prompt_stage2.format(ai_comment=ai_comment)
        raw_output = llm([HumanMessage(content=result_prompt)]).content
        result = output_parser.parse(raw_output)
        ai_hyouka = result.get("AIè©•ä¾¡", "")
        kaizen = result.get("æ”¹å–„æ¡ˆ", "")
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ (è©•ä¾¡: {ai_hyouka})")
        tokens_prompt2 = count_tokens(result_prompt)
        tokens_response2 = count_tokens(raw_output)
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”Ÿæˆã¾ãŸã¯è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        ai_hyouka = "-"
        kaizen = f"ã‚¨ãƒ©ãƒ¼: {e}"
        tokens_prompt2 = 0
        tokens_response2 = 0
    
    total_tokens = tokens_prompt1 + tokens_response1 + tokens_prompt2 + tokens_response2
    token_info = (f"Stage1: prompt {tokens_prompt1}, response {tokens_response1}; "
                  f"Stage2: prompt {tokens_prompt2}, response {tokens_response2}; "
                  f"åˆè¨ˆ: {total_tokens}")
    
    return {
        "é …ç•ª": item_no,
        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": ai_comment,
        "AIè©•ä¾¡": ai_hyouka,
        "æ”¹å–„æ¡ˆ": kaizen,
        "Tokenæ¶ˆè²»": token_info
    }

# 5. ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’åˆ†å‰²ãƒãƒƒãƒã§å‡¦ç†ã—ã¾ã™
logging.info("ãƒã‚§ãƒƒã‚¯é …ç›®ã®ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
items = df.to_dict(orient="records")
results = []
total_tasks = len(items)
processed_count = 0
batch_index = 0

while processed_count < total_tasks:
    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ±ºå®šï¼šåˆå›ã¯3å€‹ã€ä»¥é™ã¯5å€‹ãšã¤
    if batch_index == 0:
        batch_size = min(3, total_tasks - processed_count)
    else:
        batch_size = min(5, total_tasks - processed_count)
    
    batch_items = items[processed_count: processed_count + batch_size]
    logging.info(f"ãƒãƒƒãƒ {batch_index+1} ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆã‚¿ã‚¹ã‚¯æ•°: {batch_size}ï¼‰")
    
    batch_completed = 0
    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(process_check_item, item) for item in batch_items]
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            batch_completed += 1
            current_progress = processed_count + batch_completed
            print(f"é€²æ—: {current_progress}/{total_tasks} å®Œäº†")
    
    processed_count += batch_size
    if processed_count < total_tasks:
        print("æ¬¡ã®ãƒãƒƒãƒå‡¦ç†ã¾ã§20ç§’å¾…æ©Ÿä¸­...")
        time.sleep(20)
    
    batch_index += 1

logging.info("ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

result_df = pd.DataFrame(results)
try:
    result_df["é …ç•ª"] = result_df["é …ç•ª"].astype(int)
except:
    pass
if "é …ç•ª" in df.columns:
    result_df = result_df.set_index("é …ç•ª").loc[df["é …ç•ª"]].reset_index()

# 6. çµæœã‚’å…ƒã® DataFrame ã«çµ±åˆã—ã€Excel ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã™
for col in ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ", "Tokenæ¶ˆè²»"]:
    df[col] = df["é …ç•ª"].map(result_df.set_index("é …ç•ª")[col])

output_file = "checklist.xlsx"
try:
    df.to_excel(output_file, index=False, encoding="utf-8")
    logging.info(f"çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Excel ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    raise

print("æ¤œæŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯", output_file, "ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

