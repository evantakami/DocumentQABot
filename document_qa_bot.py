import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)
import os
import logging
import pandas as pd
import tiktoken
import time
import threading
import openpyxl
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils import get_column_letter
from excel_to_markdown import process_excel_file
import gradio as gr
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser
from langchain.schema import HumanMessage, ResponseSchema

# é…ç½®æ—¥å¿—
logging.basicConfig(
    filename="process.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8"
)

# ã€ä¿®æ”¹ã€‘æ–°å¢æ¨¡å‹é…ç½®ä¸è´¹ç”¨å‚æ•°
MODEL_CONFIG = {
    "GPT3.5": {
        "deployment_name": "gpt3.5-deployment",
        "input_cost": 0.000002,
        "output_cost": 0.000002
    },
    "4omini": {
        "deployment_name": "den-share-openai-gpt4o-mini",
        "input_cost": 0.000005,
        "output_cost": 0.000005
    }
}

# Azure OpenAI é…ç½®ï¼ˆå…¶å®ƒç¯å¢ƒå˜é‡ä¿æŒä¸å˜ï¼‰
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = "your_api_key"
os.environ["OPENAI_API_BASE"] = "your_api_base"
os.environ["OPENAI_API_VERSION"] = "your_api_version"

# è®¡ç®— Token æ•°é‡
def count_tokens(text, model="gpt-4o-mini"):
    """
    æŒ‡å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã® token æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        enc = tiktoken.encoding_for_model(model)
    except Exception:
        enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

first_log_done = False
first_log_lock = threading.Lock()

# å®šä¹‰æ¨¡æ¿
template_stage1 = """
ã‚ãªãŸã¯æ–‡æ›¸ãƒã‚§ãƒƒã‚¯ã®å°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å•é¡ŒãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
å•é¡ŒãŒã‚ã‚Œã°æ”¹å–„ææ¡ˆã‚‚è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
ã€ãƒã‚§ãƒƒã‚¯é …ç›®ã€‘
ç¨®åˆ¥: {type_main}
ç¨®é¡: {type_sub}
ç¢ºèªå†…å®¹: {check_item}
å…·ä½“ä¾‹/æ”¹å–„ä¾‹: {example}
æ‰‹é †ä½œæˆè€…ã‚³ãƒ¡ãƒ³ãƒˆ: {creator_comment}
ã€æ–‡æ›¸å†…å®¹ã€‘
{document}
"""

prompt_stage1 = PromptTemplate(
    input_variables=["type_main", "type_sub", "check_item", "example", "creator_comment", "document"],
    template=template_stage1
)

response_schemas = [
    ResponseSchema(name="AIè©•ä¾¡", description="è©•ä¾¡çµæœã€‚OKã€NGã€ã„ãšã‚Œã‹ã€‚"),
    ResponseSchema(name="æ”¹å–„æ¡ˆ", description="NGã®é …ç›®ã«å¯¾ã™ã‚‹ä¿®æ­£ææ¡ˆã€‚OKã®å ´åˆã¯ç©ºç™½ã€‚")
]

output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

template_stage2 = """
ã‚ãªãŸã¯ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’è©•ä¾¡ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã®ã†ã¡ã€1ã¤ã®è©•ä¾¡ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚
[OK] or [NG]
{ai_comment}
"""

prompt_stage2 = PromptTemplate(
    input_variables=["ai_comment", "document"],
    partial_variables={"format_instructions": format_instructions},
    template=template_stage2
)

# ã€ä¿®æ”¹ã€‘æ›´æ–° process_check_item å‡½æ•°ï¼Œæ–°å¢å‚æ•° llm_instance ä¸ selected_modelï¼Œå¹¶è®¡ç®— tokens ä¿¡æ¯
def process_check_item(item, doc_text, llm_instance, selected_model):
    global first_log_done

    item_no = item.get("é …ç•ª", "<no-id>")
    type_main = str(item.get("ç¨®åˆ¥", ""))
    type_sub = str(item.get("ç¨®é¡", ""))
    check_item_text = str(item.get("ç¢ºèªå†…å®¹", ""))
    example = str(item.get("å®Ÿæ–½ä¾‹åŠã³æ”¹å–„ä¾‹", ""))
    creator_comment = str(item.get("æ‰‹é †ä½œæˆè€…ã‚³ãƒ¡ãƒ³ãƒˆ", ""))

    try:
        comment_prompt = prompt_stage1.format(
            type_main=type_main,
            type_sub=type_sub,
            check_item=check_item_text,
            example=example,
            creator_comment=creator_comment,
            document=doc_text
        )

        ai_comment = llm_instance([HumanMessage(content=comment_prompt)]).content
        tokens_prompt1 = count_tokens(comment_prompt, model=MODEL_CONFIG[selected_model]["deployment_name"])
        tokens_response1 = count_tokens(ai_comment, model=MODEL_CONFIG[selected_model]["deployment_name"])

        with first_log_lock:
            if not first_log_done:
                logging.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹: " + comment_prompt)
                logging.info("AIã®ã‚³ãƒ¡ãƒ³ãƒˆå†…å®¹: " + ai_comment)
                first_log_done = True

    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {
            "é …ç•ª": item_no,
            "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "AIè©•ä¾¡": "ã‚¨ãƒ©ãƒ¼",
            "æ”¹å–„æ¡ˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0
        }

    try:
        result_prompt = prompt_stage2.format(ai_comment=ai_comment, document=doc_text)
        raw_output = llm_instance([HumanMessage(content=result_prompt)]).content
        result = output_parser.parse(raw_output)

        ai_hyouka = result.get("AIè©•ä¾¡", "")
        kaizen = result.get("æ”¹å–„æ¡ˆ", "")

        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼ˆè©•ä¾¡: {ai_hyouka}ï¼‰")
        tokens_prompt2 = count_tokens(result_prompt, model=MODEL_CONFIG[selected_model]["deployment_name"])
        tokens_response2 = count_tokens(raw_output, model=MODEL_CONFIG[selected_model]["deployment_name"])

    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        ai_hyouka = "ã‚¨ãƒ©ãƒ¼"
        kaizen = f"ã‚¨ãƒ©ãƒ¼: {e}"
        tokens_prompt2 = 0
        tokens_response2 = 0

    input_tokens = tokens_prompt1 + tokens_prompt2
    output_tokens = tokens_response1 + tokens_response2
    total_tokens = input_tokens + output_tokens

    return {
        "é …ç•ª": item_no,
        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": ai_comment,
        "AIè©•ä¾¡": ai_hyouka,
        "æ”¹å–„æ¡ˆ": kaizen,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": total_tokens
    }

# ã€ä¿®æ”¹ã€‘æ›´æ–° process_uploaded_file å‡½æ•°ï¼Œå¢åŠ  selected_types, selected_model, llm_instance å‚æ•°  
# å¹¶ä¸”ï¼š  
# 1. è¯»å– CSV åä¸å†è¿‡æ»¤ï¼Œè€Œæ˜¯åœ¨å¤„ç†æ—¶åˆ¤æ–­æ˜¯å¦ç¬¦åˆ UI å‹¾é€‰  
# 2. è¾“å‡ºåˆ° Excel æ—¶ï¼ŒæŒ‰ç…§ CSV è¡Œå·ï¼ˆç¬¬ä¸€è¡Œå¯¹åº” Excel ç¬¬10è¡Œï¼Œç¬¬äºŒè¡Œå¯¹åº” Excel ç¬¬11è¡Œâ€¦â€¦ï¼‰å†™å…¥ç»“æœï¼Œæœªå¤„ç†é¡¹åœ¨ AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ ä¸­å†™å…¥â€œãƒã‚§ãƒƒã‚¯å¯¾è±¡å¤–â€
def process_uploaded_file(uploaded_excel_file, selected_types, selected_model, llm_instance):
    try:
        # ä¿å­˜ä¸Šä¼ çš„ Excel æ–‡ä»¶
        if isinstance(uploaded_excel_file, dict):
            file_name = uploaded_excel_file["name"]
            file_data = uploaded_excel_file["data"]
            os.makedirs("uploads", exist_ok=True)
            upload_path = os.path.join("uploads", file_name)
            with open(upload_path, "wb") as f:
                f.write(file_data)

        elif hasattr(uploaded_excel_file, "read"):
            file_name = os.path.basename(uploaded_excel_file.name)
            os.makedirs("uploads", exist_ok=True)
            upload_path = os.path.join("uploads", file_name)
            with open(upload_path, "wb") as f:
                f.write(uploaded_excel_file.read())
        else:
            file_name = os.path.basename(uploaded_excel_file)
            upload_path = uploaded_excel_file

        process_excel_file(upload_path)

        md_folder = os.path.join(os.path.dirname(upload_path), os.path.splitext(file_name)[0])
        if not os.path.exists(md_folder):
            return "Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚", None

        # ã€ä¿®æ”¹ã€‘è¯»å– CSVï¼Œä¿æŒå…¨éƒ¨è¡Œï¼Œä¿è¯è¾“å‡º Excel æ—¶è¡Œå·å¯¹åº”
        csv_file = "check_list.csv"
        df = pd.read_csv(csv_file, encoding="utf-8-sig")
        items = df.to_dict(orient="records")

        md_files = [f for f in os.listdir(md_folder) if f.lower().endswith(".md")]
        if not md_files:
            return "å¤‰æ›å¾Œã®ãƒ•ã‚©ãƒ«ãƒ€ã«Markdownãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚", None

        # ä¸ºæ¯ä¸ª md æ–‡ä»¶ç”Ÿæˆç»“æœï¼Œè¿™é‡Œå‡è®¾æ¯ä¸ª md æ–‡ä»¶ç”Ÿæˆä¸€ä»½ç»“æœï¼ˆå¦‚æœæœ‰å¤šä¸ªï¼Œå¯æ ¹æ®éœ€æ±‚åˆå¹¶ï¼‰
        results_dict = {}
        cost_summary = {}
        for md_filename in md_files:
            md_path = os.path.join(md_folder, md_filename)
            with open(md_path, "r", encoding="utf-8") as f:
                doc_text = f.read()

            file_results = []
            total_input_tokens = 0
            total_output_tokens = 0
            # ã€ä¿®æ”¹ã€‘æŒ‰ CSV é¡ºåºå¤„ç†æ¯ä¸ªæ£€æŸ¥é¡¹
            for item in items:
                # å¦‚æœ UI ä¸­é€‰æ‹©äº†ç§åˆ«ï¼Œä¸”å½“å‰é¡¹çš„ç§åˆ«ä¸åœ¨æ‰€é€‰åˆ—è¡¨ä¸­ï¼Œåˆ™ç›´æ¥æ ‡è®°ä¸ºâ€œãƒã‚§ãƒƒã‚¯å¯¾è±¡å¤–â€
                if selected_types and item.get("ç¨®åˆ¥", "") not in selected_types:
                    result = {
                        "é …ç•ª": item.get("é …ç•ª", ""),
                        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": "ãƒã‚§ãƒƒã‚¯å¯¾è±¡å¤–",
                        "AIè©•ä¾¡": "",
                        "æ”¹å–„æ¡ˆ": "",
                        "input_tokens": 0,
                        "output_tokens": 0,
                        "total_tokens": 0
                    }
                else:
                    result = process_check_item(item, doc_text, llm_instance, selected_model)
                file_results.append(result)
                total_input_tokens += result.get("input_tokens", 0)
                total_output_tokens += result.get("output_tokens", 0)
                time.sleep(2)  # APIè°ƒç”¨é—´éš”

            results_dict[md_filename] = file_results

            model_cost = MODEL_CONFIG.get(selected_model, {"input_cost": 0, "output_cost": 0})
            cost_input = total_input_tokens * model_cost["input_cost"]
            cost_output = total_output_tokens * model_cost["output_cost"]
            total_cost = cost_input + cost_output
            cost_summary[md_filename] = (total_input_tokens, total_output_tokens, total_cost)

            # ã€ä¿®æ”¹ã€‘å°†ç»“æœå†™å…¥ Excel æ¨¡æ¿  
            template_excel = "F-0168-2.xlsx"
            wb = openpyxl.load_workbook(template_excel)
            template_sheet_name = "å•†ç”¨ä½œæ¥­æ‰‹é †æ›¸ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ"
            if template_sheet_name not in wb.sheetnames:
                return f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚·ãƒ¼ãƒˆ[{template_sheet_name}]ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚", None
            template_sheet = wb[template_sheet_name]

            data_font = Font(name="Meiryo UI", size=10, bold=False)
            data_alignment = Alignment(horizontal="left", vertical="bottom")
            data_fill = PatternFill(fill_type="solid", fgColor="FFFFFF")
            fields = ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ"]
            start_row = 10  # CSVç¬¬ä¸€è¡Œå¯¹åº” Excel ç¬¬10è¡Œ
            start_col = 11  # ä»ç¬¬11åˆ—å¼€å§‹å†™å…¥

            # éå† file_resultsï¼Œä¸ CSV è¡Œå¯¹åº”
            for idx, result in enumerate(file_results):
                excel_row = start_row + idx
                # å†™å…¥ AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ
                cell = template_sheet.cell(row=excel_row, column=start_col)
                cell.value = result.get("AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "")
                cell.font = data_font
                cell.alignment = data_alignment
                cell.fill = data_fill

                # å†™å…¥ AIè©•ä¾¡ï¼ˆå‡è®¾åœ¨ start_col+1 åˆ—ï¼‰
                cell = template_sheet.cell(row=excel_row, column=start_col+1)
                cell.value = result.get("AIè©•ä¾¡", "")
                cell.font = data_font
                cell.alignment = data_alignment
                cell.fill = data_fill

                # å†™å…¥ æ”¹å–„æ¡ˆï¼ˆå‡è®¾åœ¨ start_col+2 åˆ—ï¼‰
                cell = template_sheet.cell(row=excel_row, column=start_col+2)
                cell.value = result.get("æ”¹å–„æ¡ˆ", "")
                cell.font = data_font
                cell.alignment = data_alignment
                cell.fill = data_fill

            # è¾“å‡º Excel æ–‡ä»¶ï¼ˆæ­¤å¤„æ¯ä¸ª md æ–‡ä»¶ç”Ÿæˆä¸€ä¸ª Excel æ–‡ä»¶ï¼Œå¯æ ¹æ®éœ€æ±‚åˆå¹¶ï¼‰
            output_excel = f"ãƒã‚§ãƒƒã‚¯çµæœ_{os.path.splitext(file_name)[0]}_{md_filename}.xlsx"
            wb.save(output_excel)
            # è¿™é‡Œä»…è®°å½•ç¬¬ä¸€ä¸ª md æ–‡ä»¶çš„ cost_summary ç”¨äºå±•ç¤º
            break

        summary = "ãƒã‚§ãƒƒã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
        summary += f"Markdownãƒ•ã‚¡ã‚¤ãƒ«æ ¼ç´ãƒ•ã‚©ãƒ«ãƒ€: {md_folder}\n"
        for md_filename, cost in cost_summary.items():
            summary += f"{md_filename}: å…¥åŠ›tokens={cost[0]}, å‡ºåŠ›tokens={cost[1]}, ã‚³ã‚¹ãƒˆ=${cost[2]:.6f}\n"
        summary += f"ä½œæˆã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«: {output_excel}\n"

        return summary, output_excel

    except Exception as e:
        return f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", None

# ã€ä¿®æ”¹ã€‘æ›´æ–° run_interface å‡½æ•°ï¼Œæ–°å¢æ¨¡å‹ä¸ç§åˆ«é€‰æ‹©å‚æ•°ï¼Œå¹¶åŠ¨æ€åˆ›å»º llm_instance
def run_interface(uploaded_excel_file, selected_model, selected_types):
    try:
        # æ ¹æ®æ‰€é€‰æ¨¡å‹åˆ›å»º LLM å®ä¾‹
        config = MODEL_CONFIG.get(selected_model)
        if config is None:
            raise ValueError("é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒç„¡åŠ¹ã§ã™ã€‚")
        llm_instance = AzureChatOpenAI(
            deployment_name=config["deployment_name"],
            openai_api_base=os.environ["OPENAI_API_BASE"],
            openai_api_version=os.environ["OPENAI_API_VERSION"],
            openai_api_key=os.environ["OPENAI_API_KEY"],
            temperature=0
        )
        results = process_uploaded_file(uploaded_excel_file, selected_types, selected_model, llm_instance)
        return results
    except Exception as e:
        return f"å…¨ä½“å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", None

# ã€ä¿®æ”¹ã€‘è¯»å– check_list.csv ä¸­çš„ç§åˆ«é€‰é¡¹ï¼Œç”¨äº UI çš„ CheckboxGroup
try:
    df_check = pd.read_csv("check_list.csv", encoding="utf-8-sig")
    types_options = df_check["ç¨®åˆ¥"].dropna().unique().tolist()
except Exception as e:
    logging.error(f"ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    types_options = []

# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## AI æ‰‹é †æ›¸ãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«")
    with gr.Row():
        input_file = gr.File(label="Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", file_types=[".xlsx", ".xlsm"])
    with gr.Row():
        # ã€ä¿®æ”¹ã€‘æ–°å¢æ¨¡å‹é€‰æ‹©æ§ä»¶
        model_selection = gr.Radio(choices=["GPT3.5", "4omini"], label="ãƒ¢ãƒ‡ãƒ«é¸æŠ", value="4omini")
        # ã€ä¿®æ”¹ã€‘æ–°å¢ç§åˆ«è¿‡æ»¤æ§ä»¶ï¼ˆç©ºé€‰è¡¨ç¤ºå…¨éƒ¨ï¼‰
        selected_types = gr.CheckboxGroup(choices=types_options, label="ãƒã‚§ãƒƒã‚¯é …ç›®ã®ç¨®åˆ¥é¸æŠ (ç©ºæ¬„ãªã‚‰å…¨ã¦å¯¾è±¡)")
    with gr.Row():
        run_btn = gr.Button("å‡¦ç†é–‹å§‹")
    with gr.Row():
        output_message = gr.Textbox(label="å‡¦ç†æƒ…å ±", interactive=False, lines=10)
    with gr.Row():
        output_file = gr.File(label="çµæœã® Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", file_types=[".xlsx"])
    
    run_btn.click(fn=run_interface, inputs=[input_file, model_selection, selected_types], outputs=[output_message, output_file])

demo.launch()

