import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)
    

# ã€ä¿®æ”¹ã€‘æ–°å¢ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºæ‰¹é‡å¤„ç†å¤šä¸ª xlsx æ–‡ä»¶
def process_uploaded_files(uploaded_excel_files, selected_types, selected_model, llm_instance):

    all_summaries = []
    generated_excels = []

    if not isinstance(uploaded_excel_files, list):
        uploaded_excel_files = [uploaded_excel_files]

    for file_data in uploaded_excel_files:
        summary, excel_path = process_uploaded_file(file_data, selected_types, selected_model, llm_instance)
        all_summaries.append(summary)
        if excel_path:
            generated_excels.append(excel_path)

    combined_summary = "\n\n".join(all_summaries)


    if not generated_excels:
        return combined_summary, None

    zip_name = "batch_results.zip"
    with zipfile.ZipFile(zip_name, "w", zipfile.ZIP_DEFLATED) as zf:
        for excel_file in generated_excels:
            arcname = os.path.basename(excel_file)
            zf.write(excel_file, arcname)

    return combined_summary, zip_name

def run_interface(uploaded_excel_files, selected_model, selected_types):

    try:
        config = MODEL_CONFIG.get(selected_model)
        if config is None:
            raise ValueError("é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒç„¡åŠ¹ã§ã™ã€‚")

        llm_instance = AzureChatOpenAI(
            deployment_name=config["deployment_name"],
            openai_api_base=os.environ["OPENAI_API_BASE"],
            openai_api_version=os.environ["OPENAI_API_VERSION"],
            openai_api_key=os.environ["OPENAI_API_KEY"],
            temperature=0
        )

        # ã€ä¿®æ”¹ã€‘æ‰¹é‡å¤„ç†æ–‡ä»¶
        summary, zip_file = process_uploaded_files(uploaded_excel_files, selected_types, selected_model, llm_instance)
        return summary, zip_file
    except Exception as e:
        return f"å…¨ä½“å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", None

# è¯»å– check_list.csv ä¸­çš„ç¨®åˆ¥é€‰é¡¹ï¼Œç”¨äº UI çš„ CheckboxGroup
try:
    df_check = pd.read_csv("check_list.csv", encoding="utf-8-sig")
    types_options = df_check["ç¨®åˆ¥"].dropna().unique().tolist()
except Exception as e:
    logging.error(f"ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    types_options = []

# ã€ä¿®æ”¹ã€‘Gradioç•Œé¢ï¼šæ”¹ä¸º gr.Filesï¼Œå¯ä»¥ä¸€æ¬¡ä¸Šä¼ å¤šä¸ª xlsx
with gr.Blocks() as demo:
    gr.Markdown("## AI æ‰‹é †æ›¸ãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ« (è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆ)")
    with gr.Row():
        # ã€ä¿®æ”¹ã€‘è¿™é‡Œä½¿ç”¨ gr.Files è€Œä¸æ˜¯ gr.Fileï¼Œå¹¶å…è®¸å¤šæ–‡ä»¶
        input_files = gr.Files(label="Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¦ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", file_types=[".xlsx", ".xlsm"])
    with gr.Row():
        model_selection = gr.Radio(choices=["GPT3.5", "4omini"], label="ãƒ¢ãƒ‡ãƒ«é¸æŠ", value="4omini")
        selected_types = gr.CheckboxGroup(choices=types_options, label="ãƒã‚§ãƒƒã‚¯é …ç›®ã®ç¨®åˆ¥é¸æŠ (ç©ºæ¬„ãªã‚‰å…¨ã¦å¯¾è±¡)")
    with gr.Row():
        run_btn = gr.Button("å‡¦ç†é–‹å§‹")
    with gr.Row():
        output_message = gr.Textbox(label="å‡¦ç†æƒ…å ±", interactive=False, lines=15)
    with gr.Row():
        # è¿™é‡Œçš„ file_types=[".zip"] ä¾¿äºç”¨æˆ·ä¸‹è½½ä¸€ä¸ªå‹ç¼©åŒ…
        output_file = gr.File(label="çµæœã® Zip ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", file_types=[".zip"])
    
    # æ³¨æ„ï¼šInputsæ”¹ä¸º input_files (list)ï¼ŒOutputsä¿æŒä¸¤ä¸ªï¼šsummary + zip_file
    run_btn.click(fn=run_interface, inputs=[input_files, model_selection, selected_types], outputs=[output_message, output_file])

demo.launch()

