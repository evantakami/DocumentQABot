import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)
    
def process_excel_like_data(df):
    """
    é’ˆå¯¹ä¸€ä¸ªæ— è¡¨å¤´çš„ DataFrameï¼š
      1. æŸ¥æ‰¾åŒ…å«ã€Œé …ç•ªã€å’Œã€Œä½œæ¥­æ¦‚è¦ã€çš„è¡¨å¤´è¡Œï¼›
      2. å‰å‘å¡«å……è¯¥è¡Œä½œä¸ºæ–°åˆ—åï¼Œå¹¶å–å…¶ä¸‹æ–¹æ•°æ®ï¼›
      3. åˆ é™¤ã€Œé …ç•ªã€ä¸ã€Œä½œæ¥­æ¦‚è¦ã€ä¹‹é—´åŸå§‹è¡¨å¤´ä¸ºç©ºçš„åˆ—ï¼›
      4. å°†åŒä¸€å¤§æ ‡é¢˜ä¸‹çš„å¤šåˆ—åˆå¹¶ä¸ºå•åˆ—ï¼›
      5. ä»¥ã€Œé …ç•ªã€å‘ä¸‹å¡«å……åæŒ‰é …ç•ªåˆ†ç»„åˆå¹¶å¤šè¡Œï¼›
    è¿”å›æ•´ç†å¥½çš„ DataFrameã€‚
    """
    hdr_row = find_header_row(df, keywords=("é …ç•ª", "ä½œæ¥­æ¦‚è¦"), max_search=10)
    if hdr_row is None:
        print("åœ¨å‰ 10 è¡Œå†…æœªæ‰¾åˆ°åŒæ—¶åŒ…å«ã€é …ç•ªã€ã€ä½œæ¥­æ¦‚è¦ã€çš„è¡Œï¼Œæ— æ³•ç»§ç»­ã€‚")
        return None

    # å–å‡ºåŸå§‹è¡¨å¤´ï¼ˆä¸åšå¡«å……ï¼‰ï¼Œç”¨äºåˆ¤æ–­å“ªäº›åˆ—åŸæœ¬ä¸ºç©º
    original_header = df.iloc[hdr_row]
    # å‰å‘å¡«å……ï¼Œç”¨äºåç»­ä½œä¸ºåˆ—å
    ffilled_header = original_header.ffill()
    df_data = df.iloc[hdr_row+1:].copy()
    df_data.columns = ffilled_header

    # æ–°å¢ï¼šåˆ é™¤ã€Œé …ç•ªã€å’Œã€Œä½œæ¥­æ¦‚è¦ã€ä¹‹é—´åŸå§‹è¡¨å¤´ä¸ºç©ºçš„åˆ—
    start_index = None
    end_index = None
    # éå† ffill åçš„è¡¨å¤´ï¼Œæ‰¾åˆ°ã€Œé …ç•ªã€å’Œã€Œä½œæ¥­æ¦‚è¦ã€çš„ä½ç½®
    for i, col in enumerate(ffilled_header):
        if col == "é …ç•ª" and start_index is None:
            start_index = i
        if col == "ä½œæ¥­æ¦‚è¦" and start_index is not None and end_index is None:
            end_index = i
    # å¦‚æœæ‰¾åˆ°äº†ä¸¤ä¸ªä½ç½®ï¼Œå¹¶ä¸”äºŒè€…ä¹‹é—´æœ‰åˆ—
    if start_index is not None and end_index is not None and end_index > start_index:
        indices_to_drop = []
        # éå†è¿™ä¸¤ä¸ªä½ç½®ä¹‹é—´çš„åŸå§‹è¡¨å¤´
        for i in range(start_index+1, end_index):
            orig_val = original_header.iloc[i]
            # å¦‚æœåŸå§‹è¡¨å¤´ä¸ºç©ºï¼ˆæˆ–ä»…ä¸ºç©ºç™½å­—ç¬¦ä¸²ï¼‰ï¼Œåˆ™è®°å½•è¯¥åˆ—ç´¢å¼•
            if pd.isna(orig_val) or str(orig_val).strip() == "":
                indices_to_drop.append(i)
        # æŒ‰åˆ—ç´¢å¼•åˆ é™¤è¿™äº›åˆ—ï¼ˆæ³¨æ„ï¼šåˆ é™¤æ“ä½œä¸ä¼šå½±å“å…¶ä»–éƒ¨åˆ†çš„åˆ—ï¼‰
        df_data.drop(df_data.columns[indices_to_drop], axis=1, inplace=True)

    # ä»¥ä¸‹ä»£ç ä¿æŒåŸæœ‰é€»è¾‘ï¼Œå¯¹åŒä¸€å¤§æ ‡é¢˜ä¸‹çš„å¤šåˆ—è¿›è¡Œåˆå¹¶
    unique_titles = []
    for col in df_data.columns:
        if col not in unique_titles:
            unique_titles.append(col)

    title_to_indices = {}
    col_list = list(df_data.columns)
    for i, t in enumerate(col_list):
        title_to_indices.setdefault(t, []).append(i)

    merged_dict = {}
    for t in unique_titles:
        indices = title_to_indices[t]
        merged_dict[t] = df_data.apply(lambda row: merge_columns_in_row(row, indices, t), axis=1)
    df_merged_cols = pd.DataFrame(merged_dict)

    if "é …ç•ª" not in df_merged_cols.columns:
        print("åˆå¹¶åæœªå‘ç°ã€Œé …ç•ªã€åˆ—ï¼Œæ— æ³•è¿›è¡Œè¡Œåˆå¹¶ã€‚")
        return df_merged_cols

    df_merged_cols["é …ç•ª"] = df_merged_cols["é …ç•ª"].ffill()
    df_final = df_merged_cols.groupby("é …ç•ª", as_index=False).apply(merge_rows_in_group)
    df_final.reset_index(drop=True, inplace=True)

    return df_final

