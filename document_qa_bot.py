import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)


import os
import logging
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed

# LangChain ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆlangchain ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

# 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿: ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã¨äº‹å‰æ¤œè¨¼ MD ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™
csv_file = "checklist.csv"
md_file = "äº‹å‰æ¤œè¨¼.md"

# ãƒ­ã‚°ã®è¨­å®š: INFO ãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²ã—ã€æ—¥æœ¬èªãŒæ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã†ã«ã—ã¾ã™
logging.basicConfig(filename="process.log", level=logging.INFO,
                    format="%(asctime)s - %(levelname)s - %(message)s", encoding="utf-8")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")

# ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ CSV ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼ˆUTF-8 ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™ï¼‰
try:
    df = pd.read_csv(csv_file, encoding="utf-8-sig")  # utf-8-sig ã‚’ä½¿ç”¨ã—ã¦ BOM ã‚’ã‚µãƒãƒ¼ãƒˆ
except Exception as e:
    logging.error(f"CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# Markdown ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™
try:
    with open(md_file, "r", encoding="utf-8") as f:
        doc_text = f.read()
except Exception as e:
    logging.error(f"Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

logging.info(f"ãƒã‚§ãƒƒã‚¯é …ç›®ã®æ•°: {len(df)}")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

# 2. Azure OpenAI ã®è¨­å®š: API èªè¨¼æƒ…å ±ã¨ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã‚’è¨­å®šã—ã¾ã™
# å®Ÿéš›ã® Azure OpenAI è¨­å®šã«å¾“ã£ã¦ä»¥ä¸‹ã®å¤‰æ•°ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„
api_key = "YOUR_AZURE_OPENAI_API_KEY"
api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com/"  # Azure OpenAI ãƒªã‚½ãƒ¼ã‚¹ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
api_version = "2023-05-15"  # API ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰
deployment_name = "YOUR_DEPLOYMENT_NAME"  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåï¼ˆä¾‹: Azure ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸ GPT-4 ã¾ãŸã¯ GPT-3.5 ã®ãƒ¢ãƒ‡ãƒ«åï¼‰

# èªè¨¼æƒ…å ±ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã™ï¼ˆLangChain ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ï¼‰
os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = api_key
os.environ["OPENAI_API_BASE"] = api_base
os.environ["OPENAI_API_VERSION"] = api_version

# AzureChatOpenAI ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã™
try:
    llm = AzureChatOpenAI(deployment_name=deployment_name,
                          openai_api_base=api_base,
                          openai_api_version=api_version,
                          openai_api_key=api_key,
                          openai_api_type="azure",
                          temperature=0)
    logging.info("Azure OpenAI LLM ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Azure OpenAI ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# 3. å¤šæ®µéšæ¤œæŸ»ã«å¿…è¦ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®šç¾©ã—ã¾ã™

# ã‚¹ãƒ†ãƒ¼ã‚¸1ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å…¨ä½“ã«åŸºã¥ã„ã¦ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
template_stage1 = """ã‚ãªãŸã¯æ‰‹é †æ›¸ã®äº‹å‰æ¤œè¨¼ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é«˜åº¦ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦ã€ãã®é …ç›®ã«é–¢ã™ã‚‹AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§è¿°ã¹ã¦ãã ã•ã„ã€‚
ãƒã‚§ãƒƒã‚¯é …ç›®: {check_item}
æ–‡æ›¸å†…å®¹: \"\"\"{document}\"\"\"
AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ:"""
prompt_stage1 = PromptTemplate(
    input_variables=["check_item", "document"],
    template=template_stage1
)

# ã‚¹ãƒ†ãƒ¼ã‚¸2ã®å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼: JSON å½¢å¼ã§ã€ŒAIè©•ä¾¡ã€ã¨ã€Œæ”¹å–„æ¡ˆã€ã‚’å‡ºåŠ›ã™ã‚‹æ§‹é€ ã‚’å®šç¾©ã—ã¾ã™
response_schemas = [
    ResponseSchema(name="AIè©•ä¾¡", description="è©•ä¾¡çµæœã€‚OKã€NGã€ã¾ãŸã¯ã€Œ-ã€ã®ã„ãšã‚Œã‹ã€‚"),
    ResponseSchema(name="æ”¹å–„æ¡ˆ", description="è©•ä¾¡ãŒNGã®å ´åˆã€é …ç›®ã‚’æº€ãŸã™ãŸã‚ã®æ”¹å–„ææ¡ˆã€‚OKã‚„ã€Œ-ã€ã®å ´åˆã¯ç©ºæ–‡å­—ã€‚")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

# ã‚¹ãƒ†ãƒ¼ã‚¸2ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼šã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€æ§‹é€ åŒ–ã•ã‚ŒãŸè©•ä¾¡çµæœã¨æ”¹å–„æ¡ˆã‚’å‡ºåŠ›ã—ã¾ã™
template_stage2 = """ã‚ãªãŸã¯ãƒã‚§ãƒƒã‚¯çµæœã‚’åˆ¤å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€ãƒã‚§ãƒƒã‚¯é …ç›®ã®æœ€çµ‚è©•ä¾¡ã¨æ”¹å–„æ¡ˆã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
è©•ä¾¡ã¯ã€ŒOKã€ã€ŒNGã€ã¾ãŸã¯ã€Œ-ã€ã§è¡¨ã—ã€NGã®å ´åˆã¯æ”¹å–„æ¡ˆã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚
{format_instructions}
AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ: {ai_comment}"""
prompt_stage2 = PromptTemplate(
    input_variables=["ai_comment"],
    partial_variables={"format_instructions": format_instructions},
    template=template_stage2
)

# 4. å„ãƒã‚§ãƒƒã‚¯é …ç›®ã«å¯¾ã—ã¦ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã¨çµæœåˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚LLM ã‚’äºŒæ®µéšã§å‘¼ã³å‡ºã—ã¾ã™ã€‚
def process_check_item(item):
    """å„ãƒã‚§ãƒƒã‚¯é …ç›®ã«å¯¾ã—ã¦ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã¨çµæœåˆ¤å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚LLM ã‚’äºŒæ®µéšã§å‘¼ã³å‡ºã—ã¾ã™ã€‚"""
    item_no = item.get("é …ç•ª", "<no-id>")
    content = str(item.get("ç¢ºèªå†…å®¹", ""))  # ç¢ºèªå†…å®¹
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¸1ã® LLM ã‚’å‘¼ã³å‡ºã—ã€AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¾ã™
        comment_prompt = prompt_stage1.format(check_item=content, document=doc_text)
        ai_comment = llm([{"role": "user", "content": comment_prompt}]).content  # Chat ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ã‚¹ãƒ†ãƒ¼ã‚¸1ãŒå¤±æ•—ã—ãŸå ´åˆã€ã‚¹ãƒ†ãƒ¼ã‚¸2ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¿”ã—ã¾ã™
        return {
            "é …ç•ª": item_no,
            "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "AIè©•ä¾¡": "-",
            "æ”¹å–„æ¡ˆ": ""
        }
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€ã‚¹ãƒ†ãƒ¼ã‚¸2ã® LLM ã‚’å‘¼ã³å‡ºã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸè©•ä¾¡çµæœã‚’ç”Ÿæˆã—ã¾ã™
        result_prompt = prompt_stage2.format(ai_comment=ai_comment)
        raw_output = llm([{"role": "user", "content": result_prompt}]).content
        # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸçµæœã«ãƒ‘ãƒ¼ã‚¹ã—ã¾ã™
        result = output_parser.parse(raw_output)
        ai_hyouka = result.get("AIè©•ä¾¡", "")
        kaizen = result.get("æ”¹å–„æ¡ˆ", "")
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ (è©•ä¾¡: {ai_hyouka})")
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”Ÿæˆã¾ãŸã¯è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        # ã‚¹ãƒ†ãƒ¼ã‚¸2ãŒå¤±æ•—ã—ãŸå ´åˆã€è©•ä¾¡ã‚’ NG ã¨ã—ã€æ”¹å–„æ¡ˆã«ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¨˜éŒ²ã—ã¾ã™
        ai_hyouka = "-"
        kaizen = f"ã‚¨ãƒ©ãƒ¼: {e}"
    # æ–°ã—ã„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã—ã¾ã™
    return {
        "é …ç•ª": item_no,
        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": ai_comment,
        "AIè©•ä¾¡": ai_hyouka,
        "æ”¹å–„æ¡ˆ": kaizen
    }

# 5. ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’ä¸¦åˆ—å‡¦ç†ã—ã¾ã™
logging.info("ãƒã‚§ãƒƒã‚¯é …ç›®ã®ä¸¦åˆ—å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
items = df.to_dict(orient="records")  # DataFrame ã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã€æ¸¡ã—ã‚„ã™ãã—ã¾ã™
results = []

# ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦å„ãƒã‚§ãƒƒã‚¯é …ç›®ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¾ã™
with ThreadPoolExecutor() as executor:
    # å…¨ã‚¿ã‚¹ã‚¯ã‚’æå‡ºã—ã¾ã™
    futures = [executor.submit(process_check_item, item) for item in items]
    # ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ãŸé †ã«çµæœã‚’åé›†ã—ã¾ã™
    for future in as_completed(futures):
        result = future.result()
        results.append(result)

logging.info("ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

# çµæœã®ãƒªã‚¹ãƒˆã‚’ DataFrame ã«å¤‰æ›ã—ã€é …ç•ªã§ä¸¦ã¹æ›¿ãˆã¦å…ƒã®é †åºã¨ä¸€è‡´ã•ã›ã¾ã™
result_df = pd.DataFrame(results)
# é …ç•ªãŒæ•°å€¤ã®å ´åˆã¯ã‚½ãƒ¼ãƒˆã§ãã¾ã™ã€‚æ•°å€¤ã§ãªã„å ´åˆã¯å…ƒã®å‡ºç¾é †åºã«å¾“ã£ã¦ãƒãƒƒãƒãƒ³ã‚°ã—ã¾ã™ã€‚
try:
    result_df["é …ç•ª"] = result_df["é …ç•ª"].astype(int)
except:
    pass
if "é …ç•ª" in df.columns:
    # çµæœã® DataFrame ã‚’é …ç•ªé †ã«ä¸¦ã³æ›¿ãˆã¾ã™
    result_df = result_df.set_index("é …ç•ª").loc[df["é …ç•ª"]].reset_index()

# 6. çµæœã‚’å…ƒã® DataFrame ã«çµ±åˆã—ã€Excel ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã—ã¾ã™
# AIè©•ä¾¡ã®åˆ—ã‚’å…ƒã® DataFrame ã«çµ±åˆã—ã¾ã™
for col in ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ"]:
    df[col] = df["é …ç•ª"].map(result_df.set_index("é …ç•ª")[col])

# Excel ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã€æ—¥æœ¬èªã®æ–‡å­—ãŒæ­£ã—ãä¿å­˜ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™
output_file = "checklist.xlsx"
try:
    df.to_excel(output_file, index=False, encoding="utf-8")
    logging.info(f"çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Excel ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    raise

print("æ¤œæŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯", output_file, "ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")


