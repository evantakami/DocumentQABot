import gradio as gr
from langchain.text_splitter import MarkdownTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document
from ollama_wrapper import OllamaChatWrapper

OLLAMA_HOST = "localhost"
OLLAMA_PORT = 11434
MODEL_NAME = 'elyza:jp8b'  # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«å

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, api_key="")
ollama_chat = OllamaChatWrapper(base_url="http://localhost:11434", model_name="elyza:jp8b")

#Ragas(ragè©•ä¾¡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹)ã®å°å…¥
#Rerank Modelã®å°å…¥
conversation_history = []
retriever = None
qa_chain = None

def process_uploaded_file(file):
    global retriever, qa_chain
    
    if file is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
    
    file_path = file.name
    if not os.path.exists(file_path):
        return f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{file_path}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        file_content = f.read()
    
    
    documents = [Document(page_content=file_content)]
    
    
    text_splitter = MarkdownTextSplitter()
    docs = text_splitter.split_documents(documents)
    
    print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°ï¼š{len(docs)}")
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)
    
    return f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ {len(docs)} å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚"

def answer_question(question, use_online):
    global conversation_history, retriever, qa_chain
    
    if retriever is None:
        return "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚", "", ""
    
    retrieved_docs = retriever.get_relevant_documents(question)
    context_text = "\n".join([doc.page_content for doc in retrieved_docs])
    
    history_text = "\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history[-3:]])
    
    if use_online:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = qa_chain.run(prompt)
    else:
        prompt = f"Conversation history:\n{history_text}\n\nContext: {context_text}\n\nHuman: {question}\nAI:"
        answer = ollama_chat.generate(prompt)
    
    conversation_history.append((question, answer))
    if len(conversation_history) > 10:
        conversation_history.pop(0)

    retrieved_chunks = "\n\n".join([f"Chunk {i+1}:\n{doc.page_content[:200]}..." for i, doc in enumerate(retrieved_docs)])
    
    return answer, retrieved_chunks, format_history()

def format_history():
    return "\n\n".join([f"Human: {q}\nAI: {a}" for q, a in conversation_history])

def create_interface():
    with gr.Blocks(css="""
    body {
        background-color: #f0f2f5;
        font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
    }
    .gr-button {
        padding: 10px 20px;
        font-size: 14px;
        background-color: #f57c00;
        color: white;
        border-radius: 5px;
    }
    """) as iface:
        gr.Markdown("# ğŸ“„ **DocumentQABot**")
        gr.Markdown("ğŸ“ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¼šè©±ã—ã¾ã—ã‚‡ã†ï¼ğŸ’¬**")
        
        with gr.Row():
            file_upload = gr.File(label="ğŸ“„ **Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        
        with gr.Row():
            process_btn = gr.Button("ğŸ“¥ **ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†**")
            process_output = gr.Textbox(label="âœ… **å‡¦ç†çµæœ**", scale=2)

        with gr.Row():
            with gr.Column(scale=2):
                question_input = gr.Textbox(lines=2, placeholder="â“ **å•é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**")
                use_online = gr.Checkbox(label="ğŸŒ **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ (gpt-4o-mini)**", value=True)
                submit_btn = gr.Button("ğŸš€ **å•é¡Œã‚’é€ä¿¡**")
                answer_output = gr.Textbox(lines=10, label="ğŸ“ **å›ç­”**")
            
            with gr.Column(scale=1):
                history_output = gr.Textbox(lines=20, label="ğŸ“š **å¯¾è©±å±¥æ­´**", value="")
        
        retrieved_chunks_output = gr.Textbox(lines=5, label="ğŸ” **æ¤œç´¢ã•ã‚ŒãŸChunks**")
        
        process_btn.click(fn=process_uploaded_file, inputs=[file_upload], outputs=[process_output])
        submit_btn.click(fn=answer_question, inputs=[question_input, use_online], outputs=[answer_output, retrieved_chunks_output, history_output])


    return iface

if __name__ == "__main__":
    iface = create_interface()
    iface.launch(share=True)

import os
import logging
import time
import math
import pandas as pd
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import tiktoken  # ç”¨äº token è®¡æ•°

# LangChain ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from langchain.chat_models import AzureChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.schema import HumanMessage

# --- Token æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° ---
def count_tokens(text, model="gpt-3.5-turbo"):
    """
    æŒ‡å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆã® token æ•°ã‚’è¿”ã—ã¾ã™ã€‚
    """
    try:
        enc = tiktoken.encoding_for_model(model)
    except Exception:
        enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

# 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
csv_file = "checklist.csv"
md_file = "äº‹å‰æ¤œè¨¼.md"

logging.basicConfig(
    filename="process.log",
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    encoding="utf-8",
)
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹ã—ã¾ã™")

try:
    df = pd.read_csv(csv_file, encoding="utf-8-sig")
except Exception as e:
    logging.error(f"CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

try:
    with open(md_file, "r", encoding="utf-8") as f:
        doc_text = f.read()
except Exception as e:
    logging.error(f"Markdown ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

logging.info(f"ãƒã‚§ãƒƒã‚¯é …ç›®ã®æ•°: {len(df)}")
logging.info("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ")

# 2. Azure OpenAI ã®è¨­å®š
api_key = "YOUR_AZURE_OPENAI_API_KEY"
api_base = "https://YOUR_RESOURCE_NAME.openai.azure.com/"
api_version = "2023-05-15"
deployment_name = "YOUR_DEPLOYMENT_NAME"

os.environ["OPENAI_API_TYPE"] = "azure"
os.environ["OPENAI_API_KEY"] = api_key
os.environ["OPENAI_API_BASE"] = api_base
os.environ["OPENAI_API_VERSION"] = api_version

try:
    llm = AzureChatOpenAI(
        deployment_name=deployment_name,
        openai_api_base=api_base,
        openai_api_version=api_version,
        openai_api_key=api_key,
        openai_api_type="azure",
        temperature=0,
    )
    logging.info("Azure OpenAI LLM ã®åˆæœŸåŒ–ã«æˆåŠŸã—ã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Azure OpenAI ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    raise

# 3. å¤šæ®µéšæ¤œæŸ»ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãªã©
template_stage1 = """ã‚ãªãŸã¯æ‰‹é †æ›¸ã®äº‹å‰æ¤œè¨¼ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é«˜åº¦ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã¨æ–‡æ›¸å†…å®¹ã«åŸºã¥ã„ã¦ã€ãã®é …ç›®ã«é–¢ã™ã‚‹AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§è¿°ã¹ã¦ãã ã•ã„ã€‚
ãƒã‚§ãƒƒã‚¯é …ç›®: {check_item}
æ–‡æ›¸å†…å®¹: \"\"\"{document}\"\"\"
AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ:"""
prompt_stage1 = PromptTemplate(
    input_variables=["check_item", "document"],
    template=template_stage1
)

response_schemas = [
    ResponseSchema(name="AIè©•ä¾¡", description="è©•ä¾¡çµæœã€‚OKã€NGã€ã¾ãŸã¯ã€Œ-ã€ã®ã„ãšã‚Œã‹ã€‚"),
    ResponseSchema(name="æ”¹å–„æ¡ˆ", description="è©•ä¾¡ãŒNGã®å ´åˆã€é …ç›®ã‚’æº€ãŸã™ãŸã‚ã®æ”¹å–„ææ¡ˆã€‚OKã‚„ã€Œ-ã€ã®å ´åˆã¯ç©ºæ–‡å­—ã€‚")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()

template_stage2 = """ã‚ãªãŸã¯ãƒã‚§ãƒƒã‚¯çµæœã‚’åˆ¤å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã€ãƒã‚§ãƒƒã‚¯é …ç›®ã®æœ€çµ‚è©•ä¾¡ã¨æ”¹å–„æ¡ˆã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚
è©•ä¾¡ã¯ã€ŒOKã€ã€ŒNGã€ã¾ãŸã¯ã€Œ-ã€ã§è¡¨ã—ã€NGã®å ´åˆã¯æ”¹å–„æ¡ˆã‚‚ææ¡ˆã—ã¦ãã ã•ã„ã€‚
{format_instructions}
AIè©•ä¾¡ã‚³ãƒ¡ãƒ³ãƒˆ: {ai_comment}"""
prompt_stage2 = PromptTemplate(
    input_variables=["ai_comment"],
    partial_variables={"format_instructions": format_instructions},
    template=template_stage2
)

# 4. ãƒã‚§ãƒƒã‚¯é–¢æ•°
def process_check_item(item):
    item_no = item.get("é …ç•ª", "<no-id>")
    content = str(item.get("ç¢ºèªå†…å®¹", ""))
    # --- ã‚¹ãƒ†ãƒ¼ã‚¸1 ---
    try:
        comment_prompt = prompt_stage1.format(check_item=content, document=doc_text)
        ai_comment = llm([HumanMessage(content=comment_prompt)]).content
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        tokens_prompt1 = count_tokens(comment_prompt)
        tokens_response1 = count_tokens(ai_comment)
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ãƒ¡ãƒ³ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return {
            "é …ç•ª": item_no,
            "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": f"ã‚¨ãƒ©ãƒ¼: {e}",
            "AIè©•ä¾¡": "-",
            "æ”¹å–„æ¡ˆ": "",
            "Tokenæ¶ˆè²»": f"ã‚¨ãƒ©ãƒ¼: {e}"
        }

    # --- ã‚¹ãƒ†ãƒ¼ã‚¸2 ---
    try:
        result_prompt = template_stage2.format(ai_comment=ai_comment)
        raw_output = llm([HumanMessage(content=result_prompt)]).content
        result = output_parser.parse(raw_output)
        ai_hyouka = result.get("AIè©•ä¾¡", "")
        kaizen = result.get("æ”¹å–„æ¡ˆ", "")
        logging.info(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ (è©•ä¾¡: {ai_hyouka})")
        tokens_prompt2 = count_tokens(result_prompt)
        tokens_response2 = count_tokens(raw_output)
    except Exception as e:
        logging.error(f"é …ç•ª {item_no}: ã‚¹ãƒ†ãƒ¼ã‚¸2ã®è©•ä¾¡ç”Ÿæˆã¾ãŸã¯è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        ai_hyouka = "-"
        kaizen = f"ã‚¨ãƒ©ãƒ¼: {e}"
        tokens_prompt2 = 0
        tokens_response2 = 0

    total_tokens = tokens_prompt1 + tokens_response1 + tokens_prompt2 + tokens_response2
    token_info = (
        f"Stage1: prompt {tokens_prompt1}, response {tokens_response1}; "
        f"Stage2: prompt {tokens_prompt2}, response {tokens_response2}; "
        f"åˆè¨ˆ: {total_tokens}"
    )

    return {
        "é …ç•ª": item_no,
        "AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ": ai_comment,
        "AIè©•ä¾¡": ai_hyouka,
        "æ”¹å–„æ¡ˆ": kaizen,
        "Tokenæ¶ˆè²»": token_info
    }

# 5. åˆ†æ‰¹(5ä»¶ãšã¤) + 5ç§’å¾…æ©Ÿ + é€²æ—è¡¨ç¤º
logging.info("ãƒã‚§ãƒƒã‚¯é …ç›®ã®åˆ†å‰²å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
items = df.to_dict(orient="records")
results = []

batch_size = 5
total_batches = math.ceil(len(items) / batch_size)

print("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")

for batch_index in tqdm(range(total_batches), desc="é€²æ—", unit="batch"):
    start_idx = batch_index * batch_size
    end_idx = start_idx + batch_size
    chunk = items[start_idx:end_idx]

    # ã“ã®ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç† (æœ€å¤§5ã‚¹ãƒ¬ãƒƒãƒ‰)
    with ThreadPoolExecutor(max_workers=batch_size) as executor:
        futures = [executor.submit(process_check_item, item) for item in chunk]
        for future in as_completed(futures):
            results.append(future.result())

    # ã¾ã æ¬¡ã®ãƒãƒƒãƒãŒã‚ã‚‹å ´åˆã¯ 5 ç§’å¾…ã¤
    if batch_index < total_batches - 1:
        time.sleep(5)

logging.info("ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯é …ç›®ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")

# 6. çµæœã‚’çµ±åˆã—ã€Excel ã«å‡ºåŠ›
result_df = pd.DataFrame(results)
try:
    result_df["é …ç•ª"] = result_df["é …ç•ª"].astype(int)
except:
    pass
if "é …ç•ª" in df.columns:
    result_df = result_df.set_index("é …ç•ª").loc[df["é …ç•ª"]].reset_index()

for col in ["AIè©•ä¾¡ã®ã‚³ãƒ¡ãƒ³ãƒˆ", "AIè©•ä¾¡", "æ”¹å–„æ¡ˆ", "Tokenæ¶ˆè²»"]:
    df[col] = df["é …ç•ª"].map(result_df.set_index("é …ç•ª")[col])

output_file = "checklist.xlsx"
try:
    df.to_excel(output_file, index=False, encoding="utf-8")
    logging.info(f"çµæœã¯ {output_file} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
except Exception as e:
    logging.error(f"Excel ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    raise

print("æ¤œæŸ»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚çµæœã¯", output_file, "ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
